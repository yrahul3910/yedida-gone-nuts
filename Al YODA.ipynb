{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-Learn Your Own Data Augmentation  \n",
    "Code based on https://pytorch.org/tutorials/beginner/nn_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IssueCloseTimeData(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        super().__init__()\n",
    "        path = '.'\n",
    "        \n",
    "        df = pd.read_csv(f'{path}/{filename}.csv')\n",
    "        df.drop(['Unnamed: 0', 'bugID'], axis=1, inplace=True)\n",
    "        _df = df[['s1', 's2', 's3', 's4', 's5', 's6', 's8', 'y']]\n",
    "        _df['s70'] = df['s7'].apply(lambda x: eval(x)[0])\n",
    "        _df['s71'] = df['s7'].apply(lambda x: eval(x)[1])\n",
    "        _df['s72'] = df['s7'].apply(lambda x: eval(x)[2])\n",
    "        _df['s90'] = df['s9'].apply(lambda x: eval(x)[0])\n",
    "        _df['s91'] = df['s9'].apply(lambda x: eval(x)[1])\n",
    "        self.x = _df.drop('y', axis=1)\n",
    "        self.y = _df['y']\n",
    "        \n",
    "        for col in self.x.columns:\n",
    "            self.x[f'{col}_mean'] = np.mean(self.x[col])\n",
    "            self.x[f'{col}_std'] = 1./np.std(self.x[col])\n",
    "        \n",
    "        self.x = np.array(self.x)\n",
    "        \n",
    "        if filename == 'firefox':\n",
    "            self.y = np.where(self.y < 4, 0, 1)\n",
    "        elif filename == 'chromium':\n",
    "            self.y = np.where(self.y < 5, 0, 1)\n",
    "        else:\n",
    "            self.y = np.where(self.y < 6, 0, 1)\n",
    "        \n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x, self.y)\n",
    "        \n",
    "        self.pos_x = self.x_train[np.where(self.y_train == 1)[0]]\n",
    "        self.pos_y = self.y_train[np.where(self.y_train == 1)[0]]\n",
    "        self.neg_x = self.x_train[np.where(self.y_train == 0)[0]]\n",
    "        self.neg_y = self.y_train[np.where(self.y_train == 0)[0]]\n",
    "        \n",
    "        self.pos_x_test = self.x_test[np.where(self.y_test == 1)[0]]\n",
    "        self.pos_y_test = self.y_test[np.where(self.y_test == 1)[0]]\n",
    "        self.neg_x_test = self.x_test[np.where(self.y_test == 0)[0]]\n",
    "        self.neg_y_test = self.y_test[np.where(self.y_test == 0)[0]]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x_train.iloc[i,:], self.y[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y_train.shape[0]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return next(self.x), next(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_ratio=[1., 1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICTPositiveData:\n",
    "    def __init__(self, filename, batch_size=64):\n",
    "        self.filename = filename\n",
    "        self.bs = batch_size\n",
    "        \n",
    "        ict_data = IssueCloseTimeData(filename)\n",
    "        train_ds = TensorDataset(torch.FloatTensor(ict_data.pos_x), \n",
    "                                 torch.FloatTensor(ict_data.pos_y))\n",
    "        test_ds = TensorDataset(torch.FloatTensor(ict_data.pos_x_test),\n",
    "                                torch.FloatTensor(ict_data.pos_y_test))\n",
    "        \n",
    "        self.train_dl = DataLoader(train_ds, batch_size=self.bs, drop_last=True)\n",
    "        self.test_dl = DataLoader(test_ds, batch_size=self.bs, drop_last=True)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.train_dl, self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICTNegativeData:\n",
    "    def __init__(self, filename, batch_size=64):\n",
    "        self.filename = filename\n",
    "        self.bs = batch_size\n",
    "        \n",
    "        ict_data = IssueCloseTimeData(filename)\n",
    "        train_ds = TensorDataset(torch.FloatTensor(ict_data.neg_x), \n",
    "                                 torch.FloatTensor(ict_data.neg_y))\n",
    "        test_ds = TensorDataset(torch.FloatTensor(ict_data.neg_x_test),\n",
    "                                torch.FloatTensor(ict_data.neg_y_test))\n",
    "        \n",
    "        self.train_dl = DataLoader(train_ds, batch_size=self.bs, drop_last=True)\n",
    "        self.test_dl = DataLoader(test_ds, batch_size=self.bs, drop_last=True)\n",
    "        \n",
    "    def get_data(self):    \n",
    "        return self.train_dl, self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmenter(nn.Module):\n",
    "    def __init__(self, n_samples, ratio=1.):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.n_samples = n_samples\n",
    "        self.out_size = int(ratio * n_samples)\n",
    "        \n",
    "        self.layer1 = nn.Linear(in_features=n_samples, out_features=n_samples)\n",
    "        self.layer2 = nn.Linear(in_features=n_samples, out_features=n_samples)\n",
    "        self.augmented = nn.Linear(in_features=n_samples, out_features=self.out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        layer1_out = F.relu(self.layer1(torch.transpose(x, 0, 1)))\n",
    "        layer2_out = F.relu(self.layer2(layer1_out))\n",
    "        return torch.transpose(F.relu(self.augmented(layer2_out)), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainNet(nn.Module):\n",
    "    def __init__(self, input_shape:tuple, n_layers:int=2):\n",
    "        super().__init__()\n",
    "        self.n_features = input_shape[1]\n",
    "        self.n_samples = input_shape[0]\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        cur_in = n_features\n",
    "        cur_out = int(cur_in // 2)\n",
    "                                \n",
    "        self.layers = []\n",
    "        while True:\n",
    "            self.layers.append(nn.Linear(in_features=cur_in, out_features=cur_out))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            \n",
    "            cur_in = cur_out\n",
    "            cur_out = int(cur_in // 2)\n",
    "            \n",
    "            if cur_in == 2 or cur_in == 3:\n",
    "                break\n",
    "        \n",
    "        self.layers.append(nn.Linear(in_features=cur_in, out_features=1))\n",
    "                \n",
    "    def forward(self, x):\n",
    "        data = x\n",
    "        for layer in self.layers:\n",
    "            data = layer(data)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlYoda(nn.Module):\n",
    "    def __init__(self, input_shape, filename='firefox', augment_ratio=augment_ratio, n_layers=3):\n",
    "        \"\"\"\n",
    "        The Al YODA net.\n",
    "        \n",
    "        :param filename - str. Filename of dataset.\n",
    "        :param augment_ratio - Iterable. (positive_augment_ratio, negative_augment_ratio)\n",
    "        :param n_layers - int. Number of layers in the main network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.n_layers = n_layers\n",
    "        self.paused = False\n",
    "        \n",
    "        n_features = input_shape[1]\n",
    "        n_samples = input_shape[0]\n",
    "                        \n",
    "        # Augment the data\n",
    "        self.pos_augmenter = DataAugmenter(n_samples, ratio=augment_ratio[0])\n",
    "        self.neg_augmenter = DataAugmenter(n_samples, ratio=augment_ratio[1])\n",
    "        \n",
    "        main_net_input_shape = (self.pos_augmenter.out_size + self.neg_augmenter.out_size,\n",
    "                                n_features)\n",
    "        \n",
    "        # Get the main network\n",
    "        self.main_net = MainNet(input_shape=input_shape,\n",
    "                                n_layers=n_layers)\n",
    "    \n",
    "    def forward(self, x_pos, x_neg):                                \n",
    "        # Train the augmenters one step\n",
    "        x_pos = self.pos_augmenter.forward(x_pos)\n",
    "        x_neg = self.neg_augmenter.forward(x_neg)\n",
    "        \n",
    "        # Concatenate\n",
    "        x_concatenated = torch.cat((x_pos, x_neg), dim=0)\n",
    "        \n",
    "        # Train the main network one step\n",
    "        return self.main_net(x_concatenated)\n",
    "    \n",
    "    def toggle_pause(self):\n",
    "        self.paused = not self.paused\n",
    "        self.pos_augmenter.eval()\n",
    "        self.neg_augmenter.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "filename = 'firefox'\n",
    "pos_train_dl, pos_test_dl = ICTPositiveData(filename).get_data()\n",
    "neg_train_dl, neg_test_dl = ICTNegativeData(filename).get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = pos_train_dl.dataset[0][0].numel()\n",
    "n_pos_samples = pos_train_dl.dataset.tensors[1].numel()\n",
    "n_neg_samples = neg_train_dl.dataset.tensors[1].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlYoda(input_shape=(64, n_features))\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 36]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for x, y in pos_train_dl:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60: train_loss=0.4446515440940857 | val_loss=58.68923568725586\n",
      "Epoch 2/60: train_loss=0.444627046585083 | val_loss=58.68886947631836\n",
      "Epoch 3/60: train_loss=0.4446198046207428 | val_loss=58.68876266479492\n",
      "Epoch 4/60: train_loss=0.4446163475513458 | val_loss=58.6887321472168\n",
      "Epoch 5/60: train_loss=0.4446142911911011 | val_loss=58.68871307373047\n",
      "Epoch 6/60: train_loss=0.4333550035953522 | val_loss=57.20277786254883\n",
      "Epoch 7/60: train_loss=0.43335458636283875 | val_loss=57.20277786254883\n",
      "Epoch 8/60: train_loss=0.43335437774658203 | val_loss=57.20277786254883\n",
      "Epoch 9/60: train_loss=0.43335431814193726 | val_loss=57.20277786254883\n",
      "Epoch 10/60: train_loss=0.4333541989326477 | val_loss=57.20277786254883\n",
      "Epoch 11/60: train_loss=0.4333541691303253 | val_loss=57.20277404785156\n",
      "Epoch 12/60: train_loss=0.43335410952568054 | val_loss=57.20277404785156\n",
      "Epoch 13/60: train_loss=0.43335404992103577 | val_loss=57.20277404785156\n",
      "Epoch 14/60: train_loss=0.433353990316391 | val_loss=57.20277404785156\n",
      "Epoch 15/60: train_loss=0.433353990316391 | val_loss=57.20277404785156\n",
      "Epoch 16/60: train_loss=0.43335390090942383 | val_loss=57.20277404785156\n",
      "Epoch 17/60: train_loss=0.43335390090942383 | val_loss=57.20277404785156\n",
      "Epoch 18/60: train_loss=0.43335390090942383 | val_loss=57.20277404785156\n",
      "Epoch 19/60: train_loss=0.43335390090942383 | val_loss=57.20277404785156\n",
      "Epoch 20/60: train_loss=0.43335390090942383 | val_loss=57.20277404785156\n",
      "Epoch 21/60: train_loss=0.43335387110710144 | val_loss=57.20277404785156\n",
      "Epoch 22/60: train_loss=0.43335384130477905 | val_loss=57.20277404785156\n",
      "Epoch 23/60: train_loss=0.43335384130477905 | val_loss=57.20277404785156\n",
      "Epoch 24/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 25/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 26/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 27/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 28/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 29/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 30/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 31/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 32/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 33/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 34/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 35/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 36/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 37/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 38/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 39/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 40/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 41/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 42/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 43/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 44/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 45/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 46/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 47/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 48/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 49/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 50/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 51/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 52/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 53/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 54/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 55/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 56/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 57/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 58/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 59/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n",
      "Epoch 60/60: train_loss=0.43335381150245667 | val_loss=57.20277404785156\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    if epoch == n_epochs // 2:\n",
    "        model.toggle_pause()\n",
    "        \n",
    "    model.train()\n",
    "    for (xb_pos, _), (xb_neg, _) in zip(pos_train_dl, neg_train_dl):\n",
    "        preds = model(xb_pos, xb_neg).view(-1)\n",
    "        pos_shape = int(augment_ratio[0] * xb_pos.shape[0])\n",
    "        neg_shape = int(augment_ratio[1] * xb_neg.shape[0])\n",
    "        yb = torch.cat((torch.ones(pos_shape), torch.zeros(neg_shape)), dim=0)\n",
    "        train_loss = loss_func(preds, yb)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for (xb_pos, _), (xb_neg, _) in zip(pos_test_dl, neg_test_dl):\n",
    "            preds = model(xb_pos, xb_neg).view(-1)\n",
    "            pos_shape = int(augment_ratio[0] * xb_pos.shape[0])\n",
    "            neg_shape = int(augment_ratio[1] * xb_neg.shape[0])\n",
    "            yb = torch.cat((torch.ones(pos_shape), torch.zeros(neg_shape)), dim=0)\n",
    "            valid_loss += loss_func(preds, yb)\n",
    "\n",
    "    # Log\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}: train_loss={train_loss} | val_loss={valid_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss = 0.\n",
    "preds = []\n",
    "ys = []\n",
    "with torch.no_grad():\n",
    "    for (xb_pos, _), (xb_neg, _) in zip(pos_test_dl, neg_test_dl):\n",
    "        pos_shape = int(augment_ratio[0] * xb_pos.shape[0])\n",
    "        neg_shape = int(augment_ratio[1] * xb_neg.shape[0])\n",
    "        ys.extend(np.ones(pos_shape))\n",
    "        ys.extend(np.zeros(neg_shape))\n",
    "        preds.extend(model(xb_pos, xb_neg).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.array(ys).squeeze()\n",
    "preds = np.array(preds).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17688,), (17688,))"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape, preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raise_utils.metrics import ClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(x) / sum(np.exp(x-max(x))) <= .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "metr = ClassificationMetrics(ys, sigmoid(preds))\n",
    "metr.add_metrics(['accuracy', 'pd', 'pf', 'd2h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5223880597014925, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metr.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
