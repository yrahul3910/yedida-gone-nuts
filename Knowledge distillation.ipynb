{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# From https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "# and https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from random import randrange, choice\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import statistics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, recall_score, accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        return K.mean(\n",
    "            K.binary_crossentropy(y_true, y_pred) * weights)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../../../Dodge/data/defect/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dic = {\"ivy\": [\"ivy-1.1.csv\", \"ivy-1.4.csv\", \"ivy-2.0.csv\"],\n",
    "            \"lucene\": [\"lucene-2.0.csv\", \"lucene-2.2.csv\", \"lucene-2.4.csv\"],\n",
    "            \"poi\": [\"poi-1.5.csv\", \"poi-2.0.csv\", \"poi-2.5.csv\", \"poi-3.0.csv\"],\n",
    "            \"synapse\": [\"synapse-1.0.csv\", \"synapse-1.1.csv\", \"synapse-1.2.csv\"],\n",
    "            \"velocity\": [\"velocity-1.4.csv\", \"velocity-1.5.csv\", \"velocity-1.6.csv\"],\n",
    "            \"camel\": [\"camel-1.0.csv\", \"camel-1.2.csv\", \"camel-1.4.csv\", \"camel-1.6.csv\"],\n",
    "            \"jedit\": [\"jedit-3.2.csv\", \"jedit-4.0.csv\", \"jedit-4.1.csv\", \"jedit-4.2.csv\", \"jedit-4.3.csv\"],\n",
    "            \"log4j\": [\"log4j-1.0.csv\", \"log4j-1.1.csv\", \"log4j-1.2.csv\"],\n",
    "            \"xalan\": [\"xalan-2.4.csv\", \"xalan-2.5.csv\", \"xalan-2.6.csv\", \"xalan-2.7.csv\"],\n",
    "            \"xerces\": [\"xerces-1.2.csv\", \"xerces-1.3.csv\", \"xerces-1.4.csv\"]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    frac = sum(trainy) * 1.0 / len(trainy)\n",
    "    weights = np.array([1., 10. / frac])\n",
    "    \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_shape=(trainX.shape[1],), activation='relu'))\n",
    "    \n",
    "    n_layers = np.random.randint(2, 5)\n",
    "    for i in range(n_layers):\n",
    "        model.add(Dense(20, activation='relu'))\n",
    "        if np.random.random(1) <= 0.5:\n",
    "            model.add(BatchNormalization())\n",
    "        if np.random.random(1) <= 0.5:\n",
    "            model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=weighted_categorical_crossentropy(weights), optimizer='adam', metrics=['accuracy'])\n",
    "    # fit model\n",
    "    model.fit(trainX, trainy, epochs=20, verbose=0)\n",
    "    return model\n",
    " \n",
    "# make an ensemble prediction for multi-class classification\n",
    "def ensemble_predictions(members, testX):\n",
    "    # make predictions\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    yhats = np.array(yhats)\n",
    "    # sum across ensemble members\n",
    "    summed = np.sum(yhats, axis=0)\n",
    "    # argmax across classes\n",
    "    result = np.argmax(summed, axis=1)\n",
    "    return result\n",
    " \n",
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, testX, testy):\n",
    "    # select a subset of members\n",
    "    subset = members[:n_members]\n",
    "    print(len(subset))\n",
    "    # make prediction\n",
    "    yhat = ensemble_predictions(subset, testX)\n",
    "    # calculate accuracy\n",
    "    return accuracy_score(testy, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stats.stackexchange.com/a/217753\n",
    "def SMOTE(T, N, k):\n",
    "    \"\"\"\n",
    "    Returns (N/100) * n_minority_samples synthetic minority samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    T : array-like, shape = [n_minority_samples, n_features]\n",
    "        Holds the minority samples\n",
    "    N : percetange of new synthetic samples: \n",
    "        n_synthetic_samples = N/100 * n_minority_samples. Can be < 100.\n",
    "    k : int. Number of nearest neighbours. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    S : array, shape = [(N/100) * n_minority_samples, n_features]\n",
    "    \"\"\"    \n",
    "    n_minority_samples, n_features = T.shape\n",
    "\n",
    "    if N < 100:\n",
    "        #create synthetic samples only for a subset of T.\n",
    "        #TODO: select random minortiy samples\n",
    "        N = 100\n",
    "        pass\n",
    "\n",
    "    if (N % 100) != 0:\n",
    "        raise ValueError(\"N must be < 100 or multiple of 100\")\n",
    "\n",
    "    N = int(N // 100)\n",
    "    n_synthetic_samples = N * n_minority_samples\n",
    "    S = np.zeros(shape=(n_synthetic_samples, n_features))\n",
    "\n",
    "    #Learn nearest neighbours\n",
    "    neigh = NearestNeighbors(n_neighbors = k)\n",
    "    neigh.fit(T)\n",
    "\n",
    "    #Calculate synthetic samples\n",
    "    for i in range(n_minority_samples):\n",
    "        nn = neigh.kneighbors(T[i].reshape(1,-1), return_distance=False)\n",
    "        for n in range(N):\n",
    "            nn_index = choice(nn[0])\n",
    "            #NOTE: nn includes T[i], we don't want to select it \n",
    "            while nn_index == i:\n",
    "                nn_index = choice(nn[0])\n",
    "\n",
    "            dif = T[nn_index] - T[i]\n",
    "            gap = np.random.random()\n",
    "            S[n + i * N, :] = T[i,:] + gap * dif[:]\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_dataset(filename, epochs=10, layers=4, weighted=True):\n",
    "    paths = [os.path.join(base_path, file_name) for file_name in file_dic[filename]]\n",
    "    train_df = pd.concat([pd.read_csv(path) for path in paths[:-1]], ignore_index=True)\n",
    "    test_df = pd.read_csv(paths[-1])\n",
    "    \n",
    "    train_df, test_df = train_df.iloc[:, 3:], test_df.iloc[:, 3:]\n",
    "    train_size = train_df[\"bug\"].count()\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    df['bug'] = df['bug'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    train_data = df.iloc[:train_size, :]\n",
    "    test_data = df.iloc[train_size:, :]\n",
    "    \n",
    "    X_train = np.array(train_data[train_data.columns[:-2]])\n",
    "    y_train = np.array(train_data['bug'])\n",
    "    X_test = np.array(test_data[test_data.columns[:-2]])\n",
    "    y_test = np.array(test_data['bug'])\n",
    "    \n",
    "    frac = sum(y_train) * 1.0 / len(y_train)\n",
    "    if weighted:\n",
    "        weights = np.array([1., 10. / frac])\n",
    "    else:\n",
    "        weights = np.array([1., 1.])\n",
    "                \n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_shape=(X_train.shape[1],), activation='relu', name='layer1'))\n",
    "    \n",
    "    for i in range(layers - 2):\n",
    "        model.add(Dense(20, activation='relu', name='layer'+str(i+2)))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid', name='layer'+str(layers)))\n",
    "    model.compile(loss=weighted_categorical_crossentropy(weights), optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1, verbose=0, callbacks=[EarlyStopping(monitor='val_loss', patience=20, min_delta=0.0001)])\n",
    "    \n",
    "    y_pred = model.predict_classes(X_test)\n",
    "    \n",
    "    print('Recall:', recall_score(y_test, y_pred))\n",
    "    print('AUC:', roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    synthetic_data = SMOTE(X_train, 500, 5)\n",
    "    preds = model.predict_classes(synthetic_data)\n",
    "    \n",
    "    student = DecisionTreeClassifier()\n",
    "    student.fit(synthetic_data, preds)\n",
    "    \n",
    "    student_preds = student.predict(X_test)\n",
    "    print('Recall:', recall_score(y_test, student_preds))\n",
    "    print('AUC:', roc_auc_score(y_test, student_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.675\n",
      "AUC: 0.7445512820512821\n",
      "Recall: 0.725\n",
      "AUC: 0.7631410256410257\n"
     ]
    }
   ],
   "source": [
    "run_on_dataset('ivy', layers=5, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step: Ensemble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rubbish_map(*args, **kwargs):\n",
    "    return {\n",
    "        \"n_layers\": 5,\n",
    "        \"n_epochs\": 10\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble_on_dataset(filename, epochs=10, layers=4, weighted=True):\n",
    "    paths = [os.path.join(base_path, file_name) for file_name in file_dic[filename]]\n",
    "    train_df = pd.concat([pd.read_csv(path) for path in paths[:-1]], ignore_index=True)\n",
    "    test_df = pd.read_csv(paths[-1])\n",
    "    \n",
    "    train_df, test_df = train_df.iloc[:, 3:], test_df.iloc[:, 3:]\n",
    "    train_size = train_df[\"bug\"].count()\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    df['bug'] = df['bug'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    train_data = df.iloc[:train_size, :]\n",
    "    test_data = df.iloc[train_size:, :]\n",
    "    \n",
    "    X_train = np.array(train_data[train_data.columns[:-2]])\n",
    "    y_train = np.array(train_data['bug'])\n",
    "    X_test = np.array(test_data[test_data.columns[:-2]])\n",
    "    y_test = np.array(test_data['bug'])\n",
    "                \n",
    "    n_members = 10\n",
    "    members = [fit_model(X_train, y_train) for _ in range(n_members)]\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    y_pred = ensemble_predictions(members, X_test)\n",
    "    \n",
    "    print('Recall:', recall_score(y_test, y_pred))\n",
    "    print('AUC:', roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    synthetic_data = SMOTE(X_train, 500, 5)\n",
    "    preds = ensemble_predictions(members, synthetic_data)\n",
    "    \n",
    "    student = DecisionTreeClassifier()\n",
    "    student.fit(synthetic_data, preds)\n",
    "    \n",
    "    student_preds = student.predict(X_test)\n",
    "    print('Recall:', recall_score(y_test, student_preds))\n",
    "    print('AUC:', roc_auc_score(y_test, student_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.0\n",
      "AUC: 0.5\n",
      "Recall: 0.0\n",
      "AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "run_ensemble_on_dataset('ivy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation to forest\n",
    "\n",
    "The naive ensemble didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forest_on_dataset(filename, epochs=10, layers=4, weighted=True):\n",
    "    paths = [os.path.join(base_path, file_name) for file_name in file_dic[filename]]\n",
    "    train_df = pd.concat([pd.read_csv(path) for path in paths[:-1]], ignore_index=True)\n",
    "    test_df = pd.read_csv(paths[-1])\n",
    "    \n",
    "    train_df, test_df = train_df.iloc[:, 3:], test_df.iloc[:, 3:]\n",
    "    train_size = train_df[\"bug\"].count()\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    df['bug'] = df['bug'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    train_data = df.iloc[:train_size, :]\n",
    "    test_data = df.iloc[train_size:, :]\n",
    "    \n",
    "    X_train = np.array(train_data[train_data.columns[:-2]])\n",
    "    y_train = np.array(train_data['bug'])\n",
    "    X_test = np.array(test_data[test_data.columns[:-2]])\n",
    "    y_test = np.array(test_data['bug'])\n",
    "                \n",
    "    n_members = 21\n",
    "    members = [fit_model(X_train, y_train) for _ in range(n_members)]\n",
    "    \n",
    "    synthetic_data = SMOTE(X_train, 1000, 3)\n",
    "    preds = np.array([model.predict_classes(synthetic_data) for model in members])\n",
    "    \n",
    "    students = [DecisionTreeClassifier()] * n_members\n",
    "    for learner, pred in zip(students, preds):\n",
    "        learner.fit(synthetic_data, pred)\n",
    "        \n",
    "    student_preds = np.array([learner.predict_proba(X_test) for learner in students])\n",
    "    student_preds = np.apply_along_axis(np.argmax, 2, student_preds)\n",
    "    student_preds = np.apply_along_axis(statistics.mean, 0, student_preds)\n",
    "    print('Recall:', recall_score(y_test, student_preds))\n",
    "    print('AUC:', roc_auc_score(y_test, student_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.075\n",
      "AUC: 0.5150641025641026\n"
     ]
    }
   ],
   "source": [
    "run_forest_on_dataset('ivy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
